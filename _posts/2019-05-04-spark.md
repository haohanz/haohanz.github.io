---
layout: post
title: "Spark & Distributed Machine Learning"
subtitle: "Learning Notes of CMU Course 15-640 Distributed Systems"
date: 2019-05-04 23:45:13 -0400
background: '/img/posts/03.jpg'
---
# Table of Contents
- <a href="#spark">Spark</a>
- <a href="#ml">Distributed Machine Learning</a>

<div id="spark"/>
<hr>
## Spark
1. What is linage in spark
    - The dependencies between RDDs will be logged in a graph -- the sequence of operations -- lineage graph
    - use "toDebugString" would know the lineage graph
    - Enable fault-tolerant
        - With the lineage graph, we can recompute the missing or damaged partitions due to node failure. 
    - Alias:
        - RDD operator graphs
        - RDD dependencies graph
        - Output of applying transformations to the spark, a logical execution plan. 
    - Source: https://data-flair.training/blogs/rdd-lineage/
    
2. MapReduce: Pros and Cons
    - Pros:
        - High throughput
        - Fault-tolerance
    - Cons:
        - Response time is large (latency) due to I/O penalty, interactive data analysis is not possible
        - Iterative applications would be slow, 90% time spent on I/O and network (e.g. ML)
        - Abstraction not expressive enough, different use cases has different applications now.
        
3. What is Spark?
    - Berkeley's extensions to Hadoop
    - Goal 1: keep and share data sets in main memory
        - Which lead to the problem of fault tolerance
        - Which solved by lineage and small partitioning
    - Goal 2: Unified computation abstraction
        - Which make iterations viable(local work + message passing = BSP = Bulk synchronous parallel)
        - Which lead to the problem of how to balance ease-of-use and generality?
    
4. Spark: Pros and Cons
    - Pros
        - Fast response time
        - In-memory computation
        - Data sharing and fault tolerance
    - Cons
        - Cannot solve fine-grained operations
        - Cannot solve large datasets which cannot fit into memory task
        - Cannot solve task with requirement of very high efficiency - SIMD, or GPU
        
5. Why didn't use in-memory computation and data-sharing before?
    - Because there's not a way of efficient fault-tolerance if using in-memory computation, you have to 
        - Checkpoint often
        - Replicate data across nodes
        - Log each operation to node-local persistent storage
    - The latency is still the same.
    
6. Spark is fault-tolerance (recovery) efficient
    - Using coarse grained operations - lineage graph instead of storing the real data. 
    - In contrast to Hadoop/GFS, RDDs provide an interface based on coarse-grained transformations (e.g., map, filter and join) that apply the same operation to many (small-size partition) data items. This allows them to efficiently provide fault tolerance by logging the transformations used to build a dataset (its lineage) rather than the actual data.1 If a partition of an RDD is lost, the RDD has enough information about how it was derived from other RDDs to recompute  just that partition. Thus, lost data can be recovered, often quite quickly, without requiring costly replication.
    - Source: https://www.quora.com/What-is-the-difference-between-fine-grained-and-coarse-grained-transformation-in-context-of-Spark

7. Why Spark RDD is immutable?
    - Consistency model: same RDD caching and sharing across nodes.
    - Enables lineage
        - Recreate RDD anytime
        - Be more strict, need to be deterministic functions of input
    - Compatibility with HDFS storing interface - which allows append only. The modification(write) is not allowed.
    
8. (HDFS) GFS's fault tolerance
    - The chunk-server fault tolerance
        - Master grant lease to chunk primary (only during modification operation)
        - The lease would be revoked if file is being renamed or deleted)
        - The lease would be updated each time assigning a new one
        - The lease would be refreshed every 60 second if modification is not finished
    b. Why Lease/version number?
        - Network partition
        - Primary failed (What if primary failed?)
        - Revoke lease when expire or rename/delete file
        - Detect outdated chunk server with version number, consider the server with failed (shut down during lease renewal).
    c. The master fault tolerance
        - Use the master-slave replica
        - Use WAL
        - Log cannot be too long
        - Checkpoint during some period
        
9. Lazy evaluation:
    - 3 Operations:
        - Transformation (map)
        - Persist (cache to memory)
        - Action (reduce)
    - Execution(evaluation) is triggered by actions, not transformations (maps)
        

10. Deployment
    - Master Server
    - Cluster manager
    - worker nodes
        - Executor
            - Cache
            - Task1,…,Taskn
11. Scheduling
    - Partitioning/Cache-aware scheduling to minimize shuffles (data skew)
    
12. Problems
    - Lineage grows soooo large: manually checkpoint to HDFS
    - The immutable RDD - cannot support randomness
    - When need a lot of memory - not enough
    - Overhead (time and space) of copy data: because no mutate-in-place
        


<div id="ml"/>
<hr>
## Distributed Machine Learning
1. Problem 
- Scale out - more iterations/second - increase throughput
- The problem: Communication overhead scales badly with number of machines, would increase as # machines increase
- Solution: P2P
    - Higher network overhead because the transmission time, more hopes between nodes. But less bandwidth - so easier scale-out
    - Data center: put all user nodes under data center, which lead to the center monitoring problems.
2. (& Solution 2) Problem 
- BSP consistency - barrier, wait for stragglers. 
- Nodes can accept slightly stale state, can still converge
- N-Bounded delay BSP
    

